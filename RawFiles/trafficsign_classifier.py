# -*- coding: utf-8 -*-
"""trafficsign_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BFs-nb0pXrcH-DkPeaRCc5TYTguaOl92
"""

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign

import zipfile
with zipfile.ZipFile("gtsrb-german-traffic-sign.zip", 'r') as zip_ref:
    zip_ref.extractall("gtsrb-dataset")

import os
import shutil
import random

def train_test_split_directory_sorted(source_dir, dest_dir, split_ratio=0.8):
    """
    Splits the dataset in source_dir into train and test sets in dest_dir based on the split_ratio.
    Processes subfolders in sorted order (e.g., 0, 1, ..., 42).
    Args:
        source_dir (str): Path to the source directory containing subfolders for each class.
        dest_dir (str): Path to the destination directory where 'train' and 'test' folders will be created.
        split_ratio (float): Proportion of data to include in the train set (default is 0.8).
    """
    # Create train and test directories
    train_dir = os.path.join(dest_dir, 'train')
    test_dir = os.path.join(dest_dir, 'test')
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # Get sorted subfolder names
    subfolders = sorted(os.listdir(source_dir), key=lambda x: int(x) if x.isdigit() else x)

    # Iterate over each sorted class folder
    for class_name in subfolders:
        class_path = os.path.join(source_dir, class_name)
        if not os.path.isdir(class_path):
            continue

        # Create corresponding class folders in train and test directories
        train_class_dir = os.path.join(train_dir, class_name)
        test_class_dir = os.path.join(test_dir, class_name)
        os.makedirs(train_class_dir, exist_ok=True)
        os.makedirs(test_class_dir, exist_ok=True)

        # Get all files in the current class folder
        files = os.listdir(class_path)
        random.shuffle(files)

        # Split files into train and test sets
        split_index = int(len(files) * split_ratio)
        train_files = files[:split_index]
        test_files = files[split_index:]

        # Copy files to train and test directories
        for file_name in train_files:
            shutil.copy(os.path.join(class_path, file_name), train_class_dir)
        for file_name in test_files:
            shutil.copy(os.path.join(class_path, file_name), test_class_dir)

        print(f"Class {class_name}: {len(train_files)} train, {len(test_files)} test")

# Example usage
source_directory = "/content/gtsrb-dataset/Train"  # Replace with your source directory
destination_directory = "/content/Dataset-german"  # Replace with your destination directory
train_test_split_directory_sorted(source_directory, destination_directory, split_ratio=0.8)

from torch import nn
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from  torch import optim
from torchvision import transforms, utils, models

from collections import OrderedDict
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

print(device)

data_dir = "/content/Dataset-german/"

train_transforms = transforms.Compose([
                                transforms.RandomRotation(30),
                                transforms.RandomHorizontalFlip(),
                                transforms.Resize((224,224)),
                                transforms.ToTensor(),
                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

test_transforms = transforms.Compose([
                                transforms.Resize((224,224)),
                                transforms.ToTensor(),
                                transforms.Normalize([0.485, 0.456, 0.406],
                                [0.229, 0.224, 0.225])])

train_data = datasets.ImageFolder(data_dir + 'train', transform=train_transforms)
test_data = datasets.ImageFolder(data_dir + 'test', transform=test_transforms)

trainloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)
testloader = torch.utils.data.DataLoader(dataset=test_data, batch_size=64,shuffle=True)

model =  models.resnet18(pretrained=True).to(device)
#print(model)

import torch.nn.functional as F
from torch import nn

class Dc_model(nn.Module):
    def __init__(self):
        super().__init__()
        # Fully connected layers
        self.linear1 = nn.Linear(512, 256)
        self.linear2 = nn.Linear(256,43)

    def forward(self, x):
        # First layer with ReLU, batch norm, and dropout
        x = F.relu(self.linear1(x))
        x = self.linear2(x)

        return x

model_ = Dc_model().to(device)
model_

model.fc = model_

from tqdm import tqdm  # Progress bar library
import torch.optim as optim
import torch.nn as nn
import torch
import matplotlib.pyplot as plt

# Assuming `model`, `trainloader`, `testloader`, and `device` are already defined
for param in model.parameters():
    param.requires_grad = True
for param in model.fc.parameters():
    param.requires_grad = True

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

train_loss = []
val_loss = []

epochs = 30

for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")

    # Training Phase
    model.train()
    running_loss = 0.0
    running_score = 0.0
    train_loop = tqdm(trainloader, desc="Training", leave=False)  # Training progress bar
    for image, label in train_loop:
        image = image.to(device)
        label = label.to(device)

        optimizer.zero_grad()
        y_pred = model(image)
        loss = criterion(y_pred, label)
        loss.backward()  # Calculate derivatives
        optimizer.step()  # Update parameters

        val, index_ = torch.max(y_pred, axis=1)
        running_score += torch.sum(index_ == label.data).item()
        running_loss += loss.item()

        # Update progress bar
        train_loop.set_postfix(loss=loss.item(), accuracy=running_score / len(trainloader.dataset))

    epoch_score = running_score / len(trainloader.dataset)
    epoch_loss = running_loss / len(trainloader.dataset)
    train_loss.append(epoch_loss)
    print(f"Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_score:.4f}")

    # Validation Phase
    model.eval()
    running_loss = 0.0
    running_score = 0.0
    val_loop = tqdm(testloader, desc="Validating", leave=False)  # Validation progress bar
    with torch.no_grad():
        for image, label in val_loop:
            image = image.to(device)
            label = label.to(device)

            y_pred = model(image)
            loss = criterion(y_pred, label)
            running_loss += loss.item()

            val, index_ = torch.max(y_pred, axis=1)
            running_score += torch.sum(index_ == label.data).item()

            # Update progress bar
            val_loop.set_postfix(loss=loss.item(), accuracy=running_score / len(testloader.dataset))

    epoch_score = running_score / len(testloader.dataset)
    epoch_loss = running_loss / len(testloader.dataset)
    val_loss.append(epoch_loss)
    print(f"Validation Loss: {epoch_loss:.4f}, Accuracy: {epoch_score:.4f}")

    # Learning rate adjustment
    #scheduler.step(epoch_loss)

# Plot Losses
plt.plot(train_loss, label="Train Loss")
plt.plot(val_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")
plt.show()

import torch

# Assuming `model` is your ResNet with unfrozen layers and custom FC layers
model_path = "resnet_custom_fc.pth"

# Save the model's state_dict (recommended)
torch.save(model.state_dict(), model_path)

print(f"Model saved to {model_path}")